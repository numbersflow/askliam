version: '3.8'
services:
  llamacpp-server-gpu:
    image: ghcr.io/ggerganov/llama.cpp:server
    expose:
      - 8081
    volumes:
      - /home/liam/projects/models:/models
    environment:
      LLAMA_ARG_MODEL: /models/exaone-quantized.gguf
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_PORT: 8081
      LLAMA_ARG_N_GPU_LAYERS: 32
      LLAMA_ARG_THREADS: 8
      LLAMA_ARG_BATCH: 1024
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_ENDPOINT_SLOTS: 1

  redis:
    image: redis:alpine
    expose:
      - "6379"
    volumes:
      - redis_data:/data
